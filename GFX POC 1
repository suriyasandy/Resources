import streamlit as st
import polars as pl
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# =====================================================
# PAGE CONFIG
# =====================================================
st.set_page_config(
    page_title="OMRC – FX Deviation Closure Platform",
    layout="wide"
)

st.title("OMRC – Alert Closure Evidence (Historical + Daily)")
st.caption("Historical baseline + daily run | OMRC-safe")

# =====================================================
# CONFIG
# =====================================================
CACHE_DIR = Path("./parquet_cache")
CACHE_DIR.mkdir(exist_ok=True)

DATE_FORMAT = "%d/%m/%Y %H:%M:%S%.f"
MIN_PEERS = 5
MAX_PEER_TRADE_IDS = 3

peer_keys = ["ccy_pair", "product", "trade_type", "bui_value"]

# =====================================================
# HELPERS
# =====================================================
def csv_to_parquet(csv_path: Path) -> Path:
    pq = CACHE_DIR / f"{csv_path.stem}.parquet"
    if not pq.exists():
        with st.spinner(f"Converting {csv_path.name} → Parquet"):
            pl.scan_csv(csv_path, infer_schema_length=10000).collect().write_parquet(pq)
    return pq


def parse_dates(df):
    return (
        df.with_columns(
            pl.col("trade_date")
            .cast(pl.Utf8, strict=False)
            .str.strptime(pl.Datetime, DATE_FORMAT, strict=False)
            .alias("trade_datetime")
        )
        .with_columns(pl.col("trade_datetime").dt.date().alias("trade_date_only"))
    )


def compute_deviation(df):
    return df.with_columns(
        (
            (pl.col("quoted_market_rate") - pl.col("market_rate"))
            .abs() / pl.col("market_rate") * 100
        ).alias("computed_deviation_percent")
    )

# =====================================================
# INPUTS
# =====================================================
st.header("1️⃣ Historical Reference Data")

hist_path = st.text_input(
    "Historical OMRC CSV (past days / weeks)",
    placeholder="C:/data/GFX_OMRC_HISTORY.csv"
)

st.header("2️⃣ Daily OMRC File")

daily_path = st.text_input(
    "Today's OMRC CSV",
    placeholder="C:/data/GFX_OMRC_TODAY.csv"
)

if not hist_path or not daily_path:
    st.stop()

hist_path = Path(hist_path)
daily_path = Path(daily_path)

if not hist_path.exists() or not daily_path.exists():
    st.error("One or more file paths do not exist")
    st.stop()

# =====================================================
# LOAD HISTORICAL BASELINE
# =====================================================
hist_df = pl.read_parquet(csv_to_parquet(hist_path))
hist_df = parse_dates(hist_df)

hist_df = (
    hist_df
    .filter(pl.col("alert_description").is_null())
    .pipe(compute_deviation)
)

hist_baseline = (
    hist_df
    .group_by(peer_keys)
    .agg([
        pl.median("computed_deviation_percent").alias("hist_peer_median_dev"),
        pl.count().alias("hist_peer_trade_count"),
        pl.col("trade_id").head(MAX_PEER_TRADE_IDS).alias("hist_peer_trade_ids")
    ])
)

st.success("Historical baseline created")

# =====================================================
# LOAD DAILY FILE
# =====================================================
df = pl.read_parquet(csv_to_parquet(daily_path))
df = parse_dates(df)
df = compute_deviation(df)

alerts_df = df.filter(pl.col("alert_description").is_not_null())
passed_df = df.filter(pl.col("alert_description").is_null())

# =====================================================
# SAME-DAY PEERS (DAILY ONLY)
# =====================================================
same_day_peers = (
    passed_df
    .group_by(peer_keys + ["trade_date_only"])
    .agg([
        pl.median("computed_deviation_percent").alias("sd_peer_median_dev"),
        pl.count().alias("sd_peer_trade_count"),
        pl.col("trade_id").head(MAX_PEER_TRADE_IDS).alias("sd_peer_trade_ids")
    ])
)

# =====================================================
# JOIN BASELINES
# =====================================================
alerts_df = (
    alerts_df
    .join(same_day_peers, on=peer_keys + ["trade_date_only"], how="left")
    .join(hist_baseline, on=peer_keys, how="left")
)

# =====================================================
# SELECT PEER SOURCE
# =====================================================
alerts_df = alerts_df.with_columns([
    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.col("sd_peer_median_dev"))
    .otherwise(pl.col("hist_peer_median_dev"))
    .alias("peer_median_dev"),

    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.col("sd_peer_trade_ids"))
    .otherwise(pl.col("hist_peer_trade_ids"))
    .alias("peer_trade_ids"),

    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.lit("SAME_DAY"))
    .otherwise(pl.lit("HISTORICAL"))
    .alias("peer_reference_type")
])

# =====================================================
# FLATTEN PEER IDS (CSV SAFE)
# =====================================================
alerts_df = alerts_df.with_columns(
    pl.when(pl.col("peer_trade_ids").is_null())
    .then(pl.lit("N/A"))
    .otherwise(pl.col("peer_trade_ids").list.join(", "))
    .alias("peer_trade_ids_str")
)

# =====================================================
# FP-LIKE FLAG
# =====================================================
alerts_df = alerts_df.with_columns(
    pl.when(
        (pl.col("peer_median_dev").is_not_null()) &
        (pl.col("computed_deviation_percent") <= pl.col("peer_median_dev") * 1.2)
    )
    .then(1)
    .otherwise(0)
    .alias("fp_like_flag")
)

# =====================================================
# CLOSURE EVIDENCE
# =====================================================
alerts_df = alerts_df.with_columns(
    (
        pl.lit("Deviation consistent with ")
        + pl.col("peer_reference_type")
        + pl.lit(" peer behaviour. Example peer trades: ")
        + pl.col("peer_trade_ids_str")
        + pl.lit(". Observed deviation: ")
        + pl.col("computed_deviation_percent").round(2).cast(pl.Utf8)
        + pl.lit("%.")
    ).alias("closure_evidence")
)

# =====================================================
# DISPLAY
# =====================================================
st.header("Sample Closure Evidence")

st.dataframe(
    alerts_df.select([
        "trade_id",
        "trade_datetime",
        "bui_value",
        "alert_description",
        "peer_reference_type",
        "fp_like_flag",
        "closure_evidence"
    ]).head(300),
    use_container_width=True
)

# =====================================================
# CSV SAFE EXPORT (DROP NESTED DATA)
# =====================================================
csv_safe_df = alerts_df.drop([
    "sd_peer_trade_ids",
    "hist_peer_trade_ids",
    "peer_trade_ids"
])

st.download_button(
    "Download Closure Evidence (CSV)",
    csv_safe_df.write_csv(),
    file_name="omrc_daily_closure_evidence.csv",
    mime="text/csv"
)

st.success("OMRC daily analysis completed successfully")
