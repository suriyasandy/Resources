import streamlit as st
import polars as pl
import numpy as np
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# =====================================================
# PAGE CONFIG
# =====================================================
st.set_page_config(
    page_title="OMRC – Trade-Date Aware FP & Closure Evidence",
    layout="wide"
)

st.title("OMRC – Alert False Positive & Closure Evidence Platform")
st.caption(
    "Passed-trade peer reference | Same-day priority | ML for similarity only"
)

# =====================================================
# CONFIG
# =====================================================
CACHE_DIR = Path("./parquet_cache")
CACHE_DIR.mkdir(exist_ok=True)

DATE_FORMAT = "%d/%m/%Y %H:%M:%S%.f"
MIN_PEERS = 5

REQUIRED_COLS = [
    "trade_id",
    "trade_date",
    "bui_value",
    "ccy_pair",
    "product",
    "trade_type",
    "alert_description",
    "market_rate",
    "quoted_market_rate",
    "base_threshold_percent",
    "deviation_percent",
    "back_to_back_book",
    "risk_book",
]

# =====================================================
# HELPERS
# =====================================================
def convert_csv_to_parquet(csv_path: Path) -> Path:
    pq = CACHE_DIR / f"{csv_path.stem}.parquet"
    if pq.exists():
        return pq
    with st.spinner("Converting CSV → Parquet (one-time)…"):
        pl.scan_csv(csv_path, infer_schema_length=10000).collect().write_parquet(pq)
    return pq


def ensure_columns(df):
    for c in REQUIRED_COLS:
        if c not in df.columns:
            df = df.with_columns(pl.lit(None).alias(c))
    return df


def parse_trade_date(df):
    return (
        df.with_columns(
            pl.col("trade_date")
            .cast(pl.Utf8, strict=False)
            .str.strptime(pl.Datetime, DATE_FORMAT, strict=False)
            .alias("trade_datetime")
        )
        .with_columns(
            pl.col("trade_datetime").dt.date().alias("trade_date_only")
        )
    )


def compute_deviation(df):
    return df.with_columns(
        pl.when(pl.col("deviation_percent").is_null())
        .then(
            (pl.col("quoted_market_rate") - pl.col("market_rate")).abs()
            / pl.col("market_rate") * 100
        )
        .otherwise(pl.col("deviation_percent"))
        .alias("computed_deviation_percent")
    )


def derive_desk(df):
    return df.with_columns(
        pl.when(pl.col("bui_value").str.contains("BACK|BTB", case=False))
        .then(pl.lit("HEDGE_DESK"))
        .otherwise(pl.lit("CLIENT_DESK"))
        .alias("desk_type")
    )


def add_btb_flag(df):
    return df.with_columns(
        pl.when(
            (pl.col("bui_value") == "BTB") |
            (pl.col("back_to_back_book") != pl.col("risk_book"))
        )
        .then(pl.lit(1))
        .otherwise(pl.lit(0))
        .alias("is_btb_trade")
    )

# =====================================================
# INPUT
# =====================================================
st.header("1️⃣ Provide OMRC Rule Engine CSV Path")

csv_path_input = st.text_input(
    "Full path to rule-engine CSV",
    placeholder="C:/data/GFX_RULE_ENGINE_20260116.csv"
)

if not csv_path_input:
    st.stop()

csv_path = Path(csv_path_input)
if not csv_path.exists():
    st.error("File path does not exist")
    st.stop()

# =====================================================
# LOAD DATA
# =====================================================
pq_path = convert_csv_to_parquet(csv_path)
df = pl.read_parquet(pq_path)

df = ensure_columns(df)
df = parse_trade_date(df)
df = derive_desk(df)

st.success(f"Loaded {df.height:,} trades")

# =====================================================
# SPLIT ALERT / PASSED
# =====================================================
alerts_df = df.filter(pl.col("alert_description").is_not_null())
passed_df = df.filter(pl.col("alert_description").is_null())

st.metric("Alert Trades", alerts_df.height)
st.metric("Passed Trades (Peer Reference)", passed_df.height)

# =====================================================
# FEATURE ENGINEERING
# =====================================================
alerts_df = compute_deviation(alerts_df)
alerts_df = add_btb_flag(alerts_df)

alerts_df = alerts_df.with_columns(
    (pl.col("computed_deviation_percent") - pl.col("base_threshold_percent"))
    .alias("breach_amount_percent")
)

passed_df = compute_deviation(passed_df)

# =====================================================
# PEER KEYS
# =====================================================
peer_keys = ["ccy_pair", "product", "trade_type", "desk_type"]

# =====================================================
# SAME-DAY PEER STATS (PASSED TRADES)
# =====================================================
same_day_peers = (
    passed_df
    .group_by(peer_keys + ["trade_date_only"])
    .agg([
        pl.median("computed_deviation_percent").alias("sd_peer_median_dev"),
        pl.count().alias("sd_peer_trade_count")
    ])
)

# =====================================================
# HISTORICAL PEER STATS (PASSED TRADES)
# =====================================================
hist_peers = (
    passed_df
    .group_by(peer_keys)
    .agg([
        pl.median("computed_deviation_percent").alias("hist_peer_median_dev"),
        pl.count().alias("hist_peer_trade_count")
    ])
)

# =====================================================
# JOIN PEERS
# =====================================================
alerts_df = alerts_df.join(
    same_day_peers,
    on=peer_keys + ["trade_date_only"],
    how="left"
)

alerts_df = alerts_df.join(
    hist_peers,
    on=peer_keys,
    how="left"
)

# =====================================================
# SELECT PEER REFERENCE (SAME-DAY FIRST)
# =====================================================
alerts_df = alerts_df.with_columns(
    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.col("sd_peer_median_dev"))
    .otherwise(pl.col("hist_peer_median_dev"))
    .alias("peer_median_dev")
)

alerts_df = alerts_df.with_columns(
    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.col("sd_peer_trade_count"))
    .otherwise(pl.col("hist_peer_trade_count"))
    .alias("peer_trade_count")
)

alerts_df = alerts_df.with_columns(
    pl.when(pl.col("sd_peer_trade_count") >= MIN_PEERS)
    .then(pl.lit("SAME_DAY"))
    .otherwise(pl.lit("HISTORICAL"))
    .alias("peer_reference_type")
)

# =====================================================
# FALSE POSITIVE INDICATOR (EVIDENCE ONLY)
# =====================================================
alerts_df = alerts_df.with_columns(
    pl.when(
        (pl.col("peer_trade_count") >= MIN_PEERS) &
        (pl.col("computed_deviation_percent") <= pl.col("peer_median_dev") * 1.2)
    )
    .then(pl.lit(1))
    .otherwise(pl.lit(0))
    .alias("fp_like_flag")
)

# =====================================================
# CLOSURE EVIDENCE
# =====================================================
alerts_df = alerts_df.with_columns(
    (
        pl.lit("Rule: ")
        + pl.col("alert_description")
        + pl.lit(" | Deviation: ")
        + pl.col("computed_deviation_percent").round(2).cast(pl.Utf8)
        + pl.lit("% vs Threshold: ")
        + pl.col("base_threshold_percent").cast(pl.Utf8)
        + pl.lit("% | Peer median (")
        + pl.col("peer_reference_type")
        + pl.lit("): ")
        + pl.col("peer_median_dev").round(2).cast(pl.Utf8)
        + pl.lit("% | Peer count: ")
        + pl.col("peer_trade_count").cast(pl.Utf8)
        + pl.lit(" | Desk: ")
        + pl.col("desk_type")
        + pl.lit(" | BTB: ")
        + pl.col("is_btb_trade").cast(pl.Utf8)
    ).alias("closure_evidence")
)

# =====================================================
# DESK-LEVEL FALSE POSITIVE RATE
# =====================================================
st.header("2️⃣ Desk-level False Positive Rates")

desk_fp = (
    alerts_df
    .group_by("desk_type")
    .agg([
        pl.count().alias("total_alerts"),
        pl.sum("fp_like_flag").alias("fp_like_alerts")
    ])
    .with_columns(
        (pl.col("fp_like_alerts") / pl.col("total_alerts") * 100)
        .round(2)
        .alias("fp_rate_percent")
    )
)

st.dataframe(desk_fp, use_container_width=True)

# =====================================================
# ML CLUSTERING (SIMILARITY ONLY)
# =====================================================
st.header("3️⃣ Alert Clustering (ML – Similarity Only)")

cluster_features = [
    "computed_deviation_percent",
    "breach_amount_percent",
    "peer_trade_count",
    "is_btb_trade"
]

cluster_input = (
    alerts_df
    .select(["trade_id"] + cluster_features)
    .drop_nulls()
)

if cluster_input.height >= 10:
    X = cluster_input.select(cluster_features).to_numpy()
    X = StandardScaler().fit_transform(X)

    k = st.slider("Number of clusters", 2, 6, 3)

    labels = KMeans(n_clusters=k, random_state=42, n_init=10).fit_predict(X)

    cluster_df = pl.DataFrame({
        "trade_id": cluster_input["trade_id"],
        "alert_cluster": labels
    })

    alerts_df = alerts_df.join(cluster_df, on="trade_id", how="left")

    cluster_summary = (
        alerts_df
        .group_by("alert_cluster")
        .agg([
            pl.count().alias("alert_count"),
            pl.mean("computed_deviation_percent").round(2).alias("avg_deviation"),
            pl.mean("peer_trade_count").round(2).alias("avg_peer_count")
        ])
    )

    st.subheader("Cluster Summary")
    st.dataframe(cluster_summary, use_container_width=True)
else:
    st.warning("Not enough alerts for clustering")

# =====================================================
# SAMPLE VIEW
# =====================================================
st.header("4️⃣ Sample Alert Closure Evidence")

display_cols = [
    "trade_id",
    "trade_date_only",
    "desk_type",
    "alert_description",
    "computed_deviation_percent",
    "base_threshold_percent",
    "peer_reference_type",
    "peer_trade_count",
    "fp_like_flag",
    "alert_cluster",
    "closure_evidence"
]

st.dataframe(
    alerts_df.select([c for c in display_cols if c in alerts_df.columns]).head(500),
    use_container_width=True
)

# =====================================================
# EXPORT
# =====================================================
st.download_button(
    "Download Alert Closure Evidence (CSV)",
    alerts_df.write_csv(),
    file_name="omrc_alert_closure_with_trade_date.csv",
    mime="text/csv"
)

st.success("OMRC trade-date-aware false positive analysis completed")
