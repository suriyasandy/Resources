import streamlit as st
import polars as pl
from pathlib import Path
from datetime import timedelta

# =========================================================
# STREAMLIT CONFIG
# =========================================================
st.set_page_config(
    page_title="OMRC GFX â€“ Execution Context Platform",
    layout="wide"
)

st.title("ðŸ“Š OMRC â€“ GFX Execution Context Platform")
st.caption(
    "Principal-based | Polars | Historical baseline | OMRC-safe explanations"
)

# =========================================================
# SIDEBAR â€“ HISTORICAL DATA PREP
# =========================================================
st.sidebar.header("ðŸ“ Historical Data Setup")

hist_csv_path = st.sidebar.text_input(
    "Path to historical GFX CSV (week-level)",
    placeholder="/data/omrc/gfx_history_week.csv"
)

PARQUET_DIR = Path("./parquet_cache")
PARQUET_DIR.mkdir(exist_ok=True)

hist_parquet_path = None

if hist_csv_path:
    hist_csv_path = Path(hist_csv_path)

    if not hist_csv_path.exists():
        st.sidebar.error("Historical CSV file not found.")
    else:
        hist_parquet_path = PARQUET_DIR / f"{hist_csv_path.stem}.parquet"

        if hist_parquet_path.exists():
            st.sidebar.success("Cached Parquet found.")
        else:
            with st.sidebar.spinner("Converting historical CSV â†’ Parquet (one-time)..."):
                hist_df = pl.scan_csv(hist_csv_path, infer_schema_length=10000).collect()
                hist_df.write_parquet(hist_parquet_path)
            st.sidebar.success("Historical data converted to Parquet.")

if not hist_parquet_path:
    st.stop()

hist_df = pl.read_parquet(hist_parquet_path)

# =========================================================
# UPLOAD LATEST FILE
# =========================================================
st.header("ðŸ“¤ Upload Latest GFX Trade File")

latest_file = st.file_uploader(
    "Upload latest GFX CSV or Parquet",
    type=["csv", "parquet"]
)

if not latest_file:
    st.stop()

if latest_file.name.endswith(".parquet"):
    df = pl.read_parquet(latest_file)
else:
    df = pl.scan_csv(latest_file, infer_schema_length=10000).collect()

# =========================================================
# NORMALISATION
# =========================================================
def normalise(df: pl.DataFrame) -> pl.DataFrame:
    df = df.with_columns([
        pl.col("trade_date")
          .str.strptime(pl.Datetime, strict=False)
          .alias("trade_ts"),
        (pl.col("base_currency_cd") + "/" + pl.col("orig_currency_cd"))
          .alias("ccy_pair"),
    ])

    df = df.with_columns([
        pl.col("trade_ts").dt.date().alias("trade_date_only"),
        pl.col("tenor").fill_null("UNKNOWN"),
        pl.when(pl.col("base_currency_prin_amount") < 25_000_000)
          .then("Small")
          .when(pl.col("base_currency_prin_amount") < 100_000_000)
          .then("Medium")
          .otherwise("Large")
          .alias("principal_bucket"),
        pl.col("bui_value")
          .cast(pl.Utf8)
          .str.to_uppercase()
          .is_in(["BTB", "BACKTOBACK", "BACK_TO_BACK"])
          .alias("btb_flag")
    ])

    return df

df = normalise(df)
hist_df = normalise(hist_df)

# =========================================================
# PORTFOLIO CLUSTERING (TODAY)
# =========================================================
cluster_df = (
    df.groupby(["book", "trade_date_only", "ccy_pair"])
      .count()
      .rename({"count": "same_day_book_trades"})
)

df = df.join(
    cluster_df,
    on=["book", "trade_date_only", "ccy_pair"]
)

# =========================================================
# HISTORICAL BASELINE (LAST 5 BUSINESS DAYS)
# =========================================================
max_date = df.select(pl.col("trade_date_only").max()).item()
baseline_start = max_date - timedelta(days=5)

baseline = (
    hist_df.filter(pl.col("trade_date_only") < max_date)
           .filter(pl.col("trade_date_only") >= baseline_start)
           .groupby(["book", "ccy_pair"])
           .agg(
               pl.col("base_currency_prin_amount")
               .median()
               .alias("hist_median_principal")
           )
)

df = df.join(
    baseline,
    on=["book", "ccy_pair"],
    how="left"
)

df = df.with_columns(
    pl.when(pl.col("base_currency_prin_amount") > pl.col("hist_median_principal"))
      .then("Higher than recent executions")
      .otherwise("In line with recent executions")
      .alias("historical_size_context")
)

# =========================================================
# FILTERS
# =========================================================
st.sidebar.header("Filters")

book_filter = st.sidebar.multiselect(
    "Book",
    df.select("book").unique().to_series().to_list()
)

if book_filter:
    df = df.filter(pl.col("book").is_in(book_filter))

# =========================================================
# OVERVIEW TABLE
# =========================================================
st.subheader("ðŸ“‹ OMRC GFX Alert Overview")

st.dataframe(
    df.select([
        "trade_id",
        "ccy_pair",
        "product",
        "tenor",
        "principal_bucket",
        "same_day_book_trades",
        "historical_size_context",
        "btb_flag",
        "deviation_percent"
    ]).to_pandas(),
    use_container_width=True
)

# =========================================================
# DRILLDOWN
# =========================================================
st.subheader("ðŸ” Trade Drilldown")

selected_trade = st.selectbox(
    "Select Trade ID",
    df.select("trade_id").unique().to_series().to_list()
)

trade = df.filter(pl.col("trade_id") == selected_trade).row(0)
trade_dict = dict(zip(df.columns, trade))

# =========================================================
# EXECUTION CONTEXT SUMMARY (STRICT LANGUAGE)
# =========================================================
st.subheader("ðŸ“ Execution Context Summary")

def explain_trade(t):
    lines = []

    lines.append(f"Trade executed in book '{t['book']}' for currency pair {t['ccy_pair']}.")

    lines.append(f"Trade size falls into the '{t['principal_bucket']}' principal category.")

    if t["same_day_book_trades"] > 1:
        lines.append(
            "Multiple trades were observed in the same book and currency on the same business date."
        )

    lines.append(t["historical_size_context"] + " for this book and currency.")

    if t["btb_flag"]:
        lines.append(
            "Trade is identified as back-to-back; internal transfer characteristics may apply."
        )

    if t.get("alert_description"):
        lines.append(
            "Alert generated based on predefined OMRC rule conditions."
        )

    lines.append(
        "This summary provides execution context only and does not assess pricing correctness."
    )

    return lines

for l in explain_trade(trade_dict):
    st.write("â€¢", l)

# =========================================================
# RAW VIEW (AUDIT FRIENDLY)
# =========================================================
with st.expander("ðŸ”Ž View Full Trade Record (Audit / Ops)"):
    st.json(trade_dict)
