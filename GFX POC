import streamlit as st
import pandas as pd
import numpy as np
import shap
import joblib
import os
import matplotlib.pyplot as plt
from datetime import datetime

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# =========================================================
# PAGE CONFIG
# =========================================================
st.set_page_config(
    page_title="OMRC GFX Explainability Platform",
    layout="wide"
)

st.title("üìä OMRC ‚Äì GFX Explainability & Prioritisation Platform")
st.caption(
    "OMRC-only | Execution-quality context | No trade linkage | No auto-closure"
)

# =========================================================
# MODE SELECTION (CRITICAL FOR GOVERNANCE)
# =========================================================
st.sidebar.header("Application Mode")

mode = st.sidebar.radio(
    "Select Mode",
    ["Inference (Default)", "Training (Admin Only)"]
)

# =========================================================
# FILE UPLOAD
# =========================================================
file = st.file_uploader("Upload OMRC GFX Dataset (CSV)", type=["csv"])
if not file:
    st.warning("Please upload OMRC GFX data to continue.")
    st.stop()

df = pd.read_csv(file)

# =========================================================
# DATETIME HANDLING
# =========================================================
df["omrctradeTradedate"] = pd.to_datetime(df["omrctradeTradedate"])
df["trade_date"] = df["omrctradeTradedate"].dt.date

# =========================================================
# PRODUCT NORMALISATION
# =========================================================
PRODUCT_MAP = {
    "FXF": "FX Forward",
    "FXFO": "FX Forward",
    "FWD": "FX Forward",
    "FXS": "FX Swap",
    "SPOT": "FX Spot",
    "SPT": "FX Spot"
}

df["product_group"] = df["omrctradeProductsubtype"].map(PRODUCT_MAP)
df = df[df["product_group"].isin(["FX Forward", "FX Swap"])]

# =========================================================
# TENOR BUCKETING
# =========================================================
TENOR_BUCKETS = {
    "ON": "Spot / Very Short", "TN": "Spot / Very Short",
    "SN": "Spot / Very Short", "SPT": "Spot / Very Short",
    "2W": "Short", "3W": "Short",
    "1M": "Short", "2M": "Short", "3M": "Short",
    "4M": "Medium", "5M": "Medium", "6M": "Medium",
    "7M": "Medium", "8M": "Medium", "9M": "Medium",
    "10M": "Medium", "11M": "Medium",
    "18M": "Long",
    "1Y": "Long", "2Y": "Long", "3Y": "Long", "4Y": "Long"
}

df["tenor_bucket"] = df["omrctradeTenor"].map(TENOR_BUCKETS).fillna("Other")

# =========================================================
# LIQUIDITY WINDOW
# =========================================================
def liquidity_window(ts):
    h = ts.hour
    if h < 6:
        return "Asia"
    elif h < 12:
        return "London"
    elif h < 17:
        return "NY"
    else:
        return "Off-hours"

df["liquidity_window"] = df["omrctradeTradedate"].apply(liquidity_window)

# =========================================================
# CURRENCY LIQUIDITY
# =========================================================
G10 = [
    "EUR/USD", "USD/JPY", "GBP/USD", "USD/CHF",
    "AUD/USD", "USD/CAD", "NZD/USD"
]

df["ccy_liquidity"] = df["omrctradeCcyPair"].apply(
    lambda x: "G10" if x in G10 else "Non-G10"
)

# =========================================================
# PRINCIPAL BUCKET
# =========================================================
df["principal_bucket"] = pd.cut(
    df["omrctradeBasecurrprincipalamt"],
    bins=[0, 25e6, 100e6, np.inf],
    labels=["Small", "Medium", "Large"]
)

# =========================================================
# PORTFOLIO CLUSTERING (BOOK + DAY + CCY)
# =========================================================
cluster_df = (
    df.groupby(["uctradeBookname", "trade_date", "omrctradeCcyPair"])
      .size()
      .reset_index(name="cluster_trade_count")
)

df = df.merge(
    cluster_df,
    on=["uctradeBookname", "trade_date", "omrctradeCcyPair"],
    how="left"
)

# =========================================================
# WEAK LABEL GENERATION (OMRC-CORRECT)
# =========================================================
def generate_weak_label(row):
    score = 0
    if row["principal_bucket"] == "Large":
        score += 1
    if row["tenor_bucket"] in ["Medium", "Long"]:
        score += 1
    if row["liquidity_window"] in ["Asia", "Off-hours"]:
        score += 1
    if row["cluster_trade_count"] >= 3:
        score += 1
    if row["ccy_liquidity"] == "Non-G10":
        score += 1
    return 1 if score >= 3 else 0

df["explainable_label"] = df.apply(generate_weak_label, axis=1)

# =========================================================
# FEATURES
# =========================================================
FEATURES = [
    "product_group",
    "tenor_bucket",
    "liquidity_window",
    "principal_bucket",
    "ccy_liquidity",
    "cluster_trade_count"
]

# =========================================================
# TRAINING MODE (ADMIN ONLY)
# =========================================================
if mode == "Training (Admin Only)":

    st.subheader("‚ö†Ô∏è Model Training ‚Äì Admin Only")

    st.info(
        "This mode trains a NEW model using weak labels derived from OMRC rules.\n\n"
        "‚Ä¢ Training is explicit\n"
        "‚Ä¢ Model is versioned\n"
        "‚Ä¢ No automatic retraining"
    )

    if st.button("Train & Save Model"):

        X = df[FEATURES]
        y = df["explainable_label"]

        preprocessor = ColumnTransformer(
            [
                ("cat", OneHotEncoder(handle_unknown="ignore"), FEATURES[:-1]),
                ("num", "passthrough", ["cluster_trade_count"]),
            ]
        )

        model = Pipeline(
            steps=[
                ("prep", preprocessor),
                ("clf", LogisticRegression(max_iter=500))
            ]
        )

        model.fit(X, y)

        version = datetime.now().strftime("%Y%m%d_%H%M")
        model_name = f"model_omrc_gfx_{version}.pkl"

        joblib.dump(model, model_name)

        st.success(f"Model trained and saved as {model_name}")

# =========================================================
# INFERENCE MODE (DEFAULT)
# =========================================================
else:
    st.subheader("üîé Inference Mode")

    model_files = [f for f in os.listdir() if f.startswith("model_omrc_gfx")]

    if not model_files:
        st.warning("No trained model found. ML scoring disabled.")
        st.stop()

    latest_model = sorted(model_files)[-1]
    model = joblib.load(latest_model)

    st.info(f"Using model: {latest_model}")

    # ML SCORE
    df["ml_explainability_score"] = model.predict_proba(df[FEATURES])[:, 1]
    df["review_priority"] = np.where(
        df["ml_explainability_score"] < 0.4,
        "High Priority",
        "Normal Priority"
    )

    # =====================================================
    # FILTERS
    # =====================================================
    st.sidebar.header("Filters")

    ccy_filter = st.sidebar.multiselect(
        "Currency Pair",
        df["omrctradeCcyPair"].unique(),
        df["omrctradeCcyPair"].unique()
    )

    book_filter = st.sidebar.multiselect(
        "Book",
        df["uctradeBookname"].dropna().unique(),
        df["uctradeBookname"].dropna().unique()
    )

    fdf = df[
        (df["omrctradeCcyPair"].isin(ccy_filter)) &
        (df["uctradeBookname"].isin(book_filter))
    ]

    # =====================================================
    # OVERVIEW
    # =====================================================
    st.subheader("üìã OMRC Alert Overview")

    st.dataframe(
        fdf[
            [
                "exceptionId",
                "omrctradeCcyPair",
                "product_group",
                "omrctradeTenor",
                "principal_bucket",
                "liquidity_window",
                "cluster_trade_count",
                "ml_explainability_score",
                "review_priority"
            ]
        ].sort_values("ml_explainability_score"),
        use_container_width=True
    )

    # =====================================================
    # DRILLDOWN
    # =====================================================
    st.subheader("üîç Trade Drilldown")

    selected_id = st.selectbox(
        "Select Exception ID",
        fdf["exceptionId"].unique()
    )

    trade = fdf[fdf["exceptionId"] == selected_id].iloc[0]

    st.json({
        "Currency Pair": trade["omrctradeCcyPair"],
        "Product": trade["product_group"],
        "Tenor": trade["omrctradeTenor"],
        "Principal Bucket": trade["principal_bucket"],
        "Liquidity Window": trade["liquidity_window"],
        "Same-Day Book Trades": int(trade["cluster_trade_count"]),
        "ML Explainability Score": round(trade["ml_explainability_score"], 3),
        "Review Priority": trade["review_priority"]
    })

    # =====================================================
    # SHAP EXPLAINABILITY
    # =====================================================
    st.subheader("üß† SHAP Explainability")

    X_row = trade[FEATURES].to_frame().T
    X_transformed = model.named_steps["prep"].transform(X_row)

    explainer = shap.Explainer(
        model.named_steps["clf"],
        model.named_steps["prep"].transform(df[FEATURES])
    )

    shap_values = explainer(X_transformed)

    fig, ax = plt.subplots()
    shap.plots.bar(shap_values[0], max_display=10, show=False)
    st.pyplot(fig)

    # =====================================================
    # OMRC EXPLANATION
    # =====================================================
    st.subheader("üìù OMRC Explanation (Auto-Draft)")

    st.info(
        f"""
        The {trade['product_group']} trade in {trade['omrctradeCcyPair']} with tenor
        {trade['omrctradeTenor']} was executed during the {trade['liquidity_window']} window.
        The trade formed part of {int(trade['cluster_trade_count'])} same-day executions
        in the same book and currency.
        Based on execution timing, tenor profile, trade size, and portfolio activity,
        the execution context appears economically explainable.
        The ML score is used solely for review prioritisation and does not represent
        a determination of misconduct or intent.
        """
    )
