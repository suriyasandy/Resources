import streamlit as st
import polars as pl
from pathlib import Path

# =====================================================
# PAGE CONFIG
# =====================================================
st.set_page_config(
    page_title="OMRC – Alert False Positive Evidence",
    layout="wide"
)

st.title("OMRC – Alert False Positive Evidence Platform")
st.caption("Large-data safe | Rule-engine aware | Ops-ready")

# =====================================================
# CONFIG
# =====================================================
DATA_DIR = Path("./parquet_cache")
DATA_DIR.mkdir(exist_ok=True)

REQUIRED_COLS = [
    "trade_id",
    "ccy_pair",
    "book",
    "product",
    "trade_type",
    "alert_description",
    "market_rate",
    "quoted_market_rate",
    "quoted_eod_rate",
    "base_threshold_percent",
    "orig_threshold_percent",
    "deviation_percent",
    "market_rate_source",
    "originating_system",
    "bui_value",
    "back_to_back_book",
    "risk_book",
]

# =====================================================
# UTILS
# =====================================================
def convert_csv_to_parquet(csv_path: Path) -> Path:
    parquet_path = DATA_DIR / f"{csv_path.stem}.parquet"

    if parquet_path.exists():
        return parquet_path

    with st.spinner("Converting CSV → Parquet (one-time)…"):
        df = pl.scan_csv(csv_path, infer_schema_length=10000)
        df.collect().write_parquet(parquet_path)

    return parquet_path


def load_parquet(path: Path) -> pl.DataFrame:
    return pl.read_parquet(path)


def ensure_columns(df: pl.DataFrame) -> pl.DataFrame:
    for c in REQUIRED_COLS:
        if c not in df.columns:
            df = df.with_columns(pl.lit(None).alias(c))
    return df


def compute_deviation(df: pl.DataFrame) -> pl.DataFrame:
    return df.with_columns(
        pl.when(pl.col("deviation_percent").is_null())
        .then(
            (pl.col("quoted_market_rate") - pl.col("market_rate")).abs()
            / pl.col("market_rate") * 100
        )
        .otherwise(pl.col("deviation_percent"))
        .alias("computed_deviation_percent")
    )


def btb_flag(df: pl.DataFrame) -> pl.DataFrame:
    return df.with_columns(
        pl.when(
            (pl.col("bui_value") == "BTB") |
            (pl.col("back_to_back_book") != pl.col("risk_book"))
        )
        .then(pl.lit("YES"))
        .otherwise(pl.lit("NO"))
        .alias("is_btb_trade")
    )

# =====================================================
# STEP 1 – CSV PATH INPUT
# =====================================================
st.header("1️⃣ Provide Rule-Engine Output File Path")

csv_path_input = st.text_input(
    "Enter FULL path to GFX rule-engine CSV file",
    placeholder="C:/data/GFX_RULE_ENGINE_20260116.csv"
)

if not csv_path_input:
    st.stop()

csv_path = Path(csv_path_input)

if not csv_path.exists():
    st.error("File path does not exist")
    st.stop()

# =====================================================
# STEP 2 – CONVERT TO PARQUET (ONCE)
# =====================================================
parquet_path = convert_csv_to_parquet(csv_path)

st.success(f"Using Parquet file: {parquet_path}")

# =====================================================
# STEP 3 – LOAD PARQUET
# =====================================================
with st.spinner("Loading Parquet data…"):
    df = load_parquet(parquet_path)

st.success(f"Loaded {df.height:,} trades")

df = ensure_columns(df)

# =====================================================
# STEP 4 – ALERT / NON-ALERT SPLIT
# =====================================================
alerts_df = df.filter(pl.col("alert_description").is_not_null())
clean_df = df.filter(pl.col("alert_description").is_null())

st.metric("Total Trades", df.height)
st.metric("Alerted Trades", alerts_df.height)
st.metric("Non-Alert Trades (Control)", clean_df.height)

# =====================================================
# STEP 5 – COMPUTE DEVIATIONS
# =====================================================
df = compute_deviation(df)

alerts_df = compute_deviation(alerts_df)
clean_df = compute_deviation(clean_df)

alerts_df = btb_flag(alerts_df)

# =====================================================
# STEP 6 – PEER GROUP STATS (NON-ALERTS ONLY)
# =====================================================
peer_keys = ["ccy_pair", "book", "product", "trade_type"]

peer_stats = (
    clean_df
    .group_by(peer_keys)
    .agg([
        pl.median("computed_deviation_percent").alias("peer_median_dev"),
        pl.quantile("computed_deviation_percent", 0.75).alias("peer_75pct_dev"),
        pl.count().alias("peer_trade_count")
    ])
)

# =====================================================
# STEP 7 – ENRICH ALERTS WITH PEER EVIDENCE
# =====================================================
alerts_enriched = alerts_df.join(
    peer_stats,
    on=peer_keys,
    how="left"
)

alerts_enriched = alerts_enriched.with_columns(
    (pl.col("computed_deviation_percent") - pl.col("base_threshold_percent"))
    .alias("breach_amount_percent")
)

# =====================================================
# STEP 8 – CLOSURE-READY EXPLANATION
# =====================================================
alerts_enriched = alerts_enriched.with_columns(
    (
        pl.lit("Rule breach: ")
        + pl.col("alert_description")
        + pl.lit(" | Deviation: ")
        + pl.col("computed_deviation_percent").round(2).cast(pl.Utf8)
        + pl.lit("% (Threshold: ")
        + pl.col("base_threshold_percent").cast(pl.Utf8)
        + pl.lit("%)")
        + pl.lit(" | Peer median (non-alert): ")
        + pl.col("peer_median_dev").round(2).cast(pl.Utf8)
        + pl.lit("% | Peer trades: ")
        + pl.col("peer_trade_count").cast(pl.Utf8)
        + pl.lit(" | BTB: ")
        + pl.col("is_btb_trade")
        + pl.lit(" | Market source: ")
        + pl.col("market_rate_source").cast(pl.Utf8)
    ).alias("closure_evidence")
)

# =====================================================
# STEP 9 – DISPLAY
# =====================================================
st.header("2️⃣ Alert-Level False-Positive Evidence")

display_cols = [
    "trade_id",
    "alert_description",
    "computed_deviation_percent",
    "base_threshold_percent",
    "breach_amount_percent",
    "peer_median_dev",
    "peer_75pct_dev",
    "peer_trade_count",
    "is_btb_trade",
    "closure_evidence",
]

st.dataframe(
    alerts_enriched.select(display_cols).head(500),
    use_container_width=True
)

# =====================================================
# STEP 10 – RULE SUMMARY
# =====================================================
st.header("3️⃣ Rule-wise Context")

rule_summary = (
    alerts_enriched
    .group_by("alert_description")
    .agg([
        pl.count().alias("alert_count"),
        pl.mean("breach_amount_percent").alias("avg_breach"),
        pl.mean("peer_trade_count").alias("avg_peer_trades")
    ])
    .sort("alert_count", descending=True)
)

st.dataframe(rule_summary, use_container_width=True)

# =====================================================
# STEP 11 – EXPORT
# =====================================================
st.download_button(
    "Download Closure Evidence (CSV)",
    alerts_enriched.select(display_cols).write_csv(),
    file_name="omrc_false_positive_evidence.csv",
    mime="text/csv"
)

st.success("OMRC false-positive analysis completed successfully")
