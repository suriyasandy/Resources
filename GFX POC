import streamlit as st
import polars as pl
from pathlib import Path

# =====================================================
# PAGE CONFIG
# =====================================================
st.set_page_config(page_title="OMRC – GFX Execution Context", layout="wide")

st.title("OMRC – GFX Execution Context Platform")
st.caption("Structural baseline | Large-file safe | OMRC-approved logic")

# =====================================================
# CONFIG
# =====================================================
DATA_DIR = Path("./data")
DATA_DIR.mkdir(exist_ok=True)

HIST_PARQUET = DATA_DIR / "historical_gfx.parquet"

DATE_FORMAT = "%d/%m/%Y %H:%M:%S%.f"

REQUIRED_COLS = [
    "trade_id",
    "book",
    "ccy_pair",
    "tenor",
    "trade_date",
    "base_currency_prin_amount",
    "bui_value"
]

# =====================================================
# HELPERS
# =====================================================
def load_any_path(path: str) -> pl.DataFrame:
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"{path} does not exist")

    if path.suffix.lower() == ".parquet":
        return pl.read_parquet(path)
    elif path.suffix.lower() == ".csv":
        return pl.read_csv(path, ignore_errors=True)
    else:
        raise ValueError("Only CSV or Parquet supported")

def ensure_columns(df: pl.DataFrame) -> pl.DataFrame:
    for c in REQUIRED_COLS:
        if c not in df.columns:
            df = df.with_columns(pl.lit(None).alias(c))
    return df

def parse_trade_date(df: pl.DataFrame) -> pl.DataFrame:
    return (
        df.with_columns(
            pl.col("trade_date")
            .cast(pl.Utf8, strict=False)
            .str.strptime(pl.Datetime, DATE_FORMAT, strict=False)
            .alias("trade_datetime")
        )
        .with_columns(
            pl.col("trade_datetime").dt.date().alias("trade_date_only")
        )
    )

# =====================================================
# BASELINE (STRUCTURAL – CORRECT)
# =====================================================
def build_baseline(df: pl.DataFrame):
    keys = ["book", "ccy_pair", "tenor"]

    baseline = (
        df.group_by(keys)
        .agg([
            pl.median("base_currency_prin_amount")
              .alias("baseline_median_principal"),
            pl.count()
              .alias("baseline_trade_count")
        ])
    )

    return baseline, keys

def explain(row):
    msgs = []

    if row["baseline_trade_count"] is None:
        return "No historical reference available for this execution"

    if row["is_btb"]:
        msgs.append("Back-to-back trade")

    if row["principal_ratio"] is not None and row["principal_ratio"] > 3:
        msgs.append("Execution notional materially larger than historical median")

    if row["baseline_trade_count"] < 5:
        msgs.append("Limited historical reference for this execution")

    if not msgs:
        msgs.append("Execution consistent with historical trading behaviour")

    return "; ".join(msgs)

# =====================================================
# SECTION 1 – HISTORICAL BASELINE
# =====================================================
st.header("1️⃣ Historical Baseline (One-time)")

hist_path = st.text_input(
    "Path to historical GFX CSV / Parquet",
    placeholder="C:/data/GFX_HBEU_20260101_20260110.csv"
)

if st.button("Prepare Historical Baseline"):
    if not hist_path:
        st.error("Please provide a valid historical file path")
    else:
        with st.spinner("Preparing historical baseline..."):
            hist_df = load_any_path(hist_path)
            hist_df = ensure_columns(hist_df)
            hist_df = parse_trade_date(hist_df)
            hist_df.write_parquet(HIST_PARQUET)

        st.success("Historical baseline prepared successfully")

# =====================================================
# STOP IF BASELINE NOT READY
# =====================================================
if not HIST_PARQUET.exists():
    st.warning("Historical baseline not found. Prepare it first.")
    st.stop()

# =====================================================
# SECTION 2 – DAILY FILE INFERENCE
# =====================================================
st.header("2️⃣ Daily GFX File Inference")

daily_path = st.text_input(
    "Path to DAILY GFX CSV / Parquet",
    placeholder="C:/data/GFX_HBEU_RIVER_20260116.csv"
)

if not daily_path:
    st.stop()

with st.spinner("Loading daily GFX file..."):
    daily_df = load_any_path(daily_path)
    daily_df = ensure_columns(daily_df)
    daily_df = parse_trade_date(daily_df)

st.success(f"Loaded {daily_df.height:,} daily trades")

# =====================================================
# LOAD BASELINE
# =====================================================
hist_df = pl.read_parquet(HIST_PARQUET)
baseline_df, baseline_keys = build_baseline(hist_df)

st.info("Baseline uses structural grouping: book + currency pair + tenor")

# =====================================================
# JOIN BASELINE (FIXED)
# =====================================================
daily_df = daily_df.join(
    baseline_df,
    on=baseline_keys,
    how="left"
)

# =====================================================
# FEATURES
# =====================================================
daily_df = daily_df.with_columns([
    (pl.col("base_currency_prin_amount") /
     pl.col("baseline_median_principal")).alias("principal_ratio"),

    (pl.col("bui_value") == "BTB").fill_null(False).alias("is_btb")
])

# =====================================================
# EXPLANATIONS
# =====================================================
daily_df = daily_df.with_columns(
    pl.struct([
        "principal_ratio",
        "baseline_trade_count",
        "is_btb"
    ]).map_elements(explain).alias("execution_explanation")
)

# =====================================================
# OUTPUT
# =====================================================
st.header("3️⃣ Execution Context Output")

display_cols = [
    "trade_id",
    "book",
    "ccy_pair",
    "tenor",
    "base_currency_prin_amount",
    "baseline_median_principal",
    "baseline_trade_count",
    "execution_explanation"
]

st.dataframe(
    daily_df.select([c for c in display_cols if c in daily_df.columns]).head(500),
    use_container_width=True
)

# =====================================================
# SUMMARY
# =====================================================
st.header("4️⃣ Summary")

st.metric(
    "Trades with historical reference",
    daily_df.filter(pl.col("baseline_trade_count").is_not_null()).height
)

st.metric(
    "Back-to-Back Trades",
    daily_df.filter(pl.col("is_btb")).height
)

st.metric(
    "Elevated Notional Trades",
    daily_df.filter(pl.col("principal_ratio") > 3).height
)

st.success("Daily inference completed successfully")
